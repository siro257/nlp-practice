{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f7e018",
   "metadata": {},
   "source": [
    "# Regex\n",
    "\n",
    "| symbol | description |\n",
    "| --- | ------------ |\n",
    "| . | any char |\n",
    "| ? | this char might have this char or not (0 or 1)|\n",
    "| * | this char appears __0 or more times__ |\n",
    "| + | this char appears __at least once__ |\n",
    "| ^ | starts with this char |\n",
    "| \\$ | ends with this char |\n",
    "| \\{n\\} | repeats `n` times |\n",
    "| \\{n1, n2\\} | repeats at least `n1` times, and at most `n2` times |\n",
    "| \\[abc \\] | this char should match any of the char in the bracket (can be range, ex. a-z)|\n",
    "| a\\|b | a or b |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c6200",
   "metadata": {},
   "source": [
    "## regex using backslash (\\\\)\n",
    "\n",
    "|symbol|description|\n",
    "|---|-----|\n",
    "|\\\\ \\\\ | backslash itself|\n",
    "| \\\\d | any digit ([0-9]) |\n",
    "| \\\\D | All char __except__ any digit ([^0-9]) |\n",
    "| \\\\s | whitespace |\n",
    "| \\\\S | All char __except__ whitespace |\n",
    "| \\\\w | any char OR digit ([a-zA-Z0-9])|\n",
    "| \\\\W | Anything __except__ all char or digit ([^a-zA-Z0-9])|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9af325d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1903279",
   "metadata": {},
   "source": [
    "# re.match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "<re.Match object; span=(0, 3), match='ab2'>\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "check = 'ab.'\n",
    "\n",
    "print(re.match(check, 'abc'))\n",
    "print(re.match(check, 'ab2'))\n",
    "print(re.match(check, 'c'))\n",
    "print(re.match(check, 'ab'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fed85c",
   "metadata": {},
   "source": [
    "# re.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed43cdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time it took:  0.0006031990051269531\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "normal_s_time = time.time()\n",
    "r = 'ab.'\n",
    "for i in range(1000):\n",
    "    re.match(r, 'abc' )\n",
    "print('time it took: ', time.time() - normal_s_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "23d58edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complie time:  0.0002760887145996094\n"
     ]
    }
   ],
   "source": [
    "compile_s_time = time.time()\n",
    "r = re.compile('ab.')\n",
    "for i in range(1000):\n",
    "    r.match('abc')\n",
    "print('complie time: ', time.time() - compile_s_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01759708",
   "metadata": {},
   "source": [
    "# re.search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9a585757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 2), match='ab'>\n",
      "<re.Match object; span=(0, 1), match='a'>\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "check = 'ab?'\n",
    "\n",
    "print(re.match('ab', check))\n",
    "print(re.search('a', check))\n",
    "\n",
    "print(re.match('kkk ab', check))\n",
    "print(re.search('kkk ab', check))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c2bc21",
   "metadata": {},
   "source": [
    "# re.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ec5ef05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaa', 'bbbb', 'cccc']\n",
      "['ab', ' abb', ' ab', 'bab']\n",
      "['s', 'abc', 'bc', '', '', '', 'ja', 'j', '', 'l', '']\n"
     ]
    }
   ],
   "source": [
    "r = re.compile(' ')\n",
    "print(r.split('aaa bbbb cccc'))\n",
    "\n",
    "r = re.compile('c')\n",
    "print(r.split('abc abbc abcbab'))\n",
    "\n",
    "r = re.compile('[0-9]')\n",
    "print(r.split('s1abc2bc2320ja1j23l4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f81c39c",
   "metadata": {},
   "source": [
    "# re.sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0973b9b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "abc defg\n"
     ]
    }
   ],
   "source": [
    "print(re.sub('[a-z]', 'abcdefg', '1'))\n",
    "\n",
    "print(re.sub('[^a-z]', 'abc defg', '1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba53a6",
   "metadata": {},
   "source": [
    "# re.findall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19195835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2']\n",
      "['!', '@', '#', '@', '#']\n",
      "['j', 'a', 'm', 'e', 's', 'j', 'a', 'm', 'e', 's', 'j', 'a', 'm', 'e', 's', 'h', 'w', 'a', 'n', 'g']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall('[\\d]', '1ab 2cd'))\n",
    "\n",
    "print(re.findall('[\\W]', '!abc@#@#Fg'))\n",
    "\n",
    "print(re.findall('\\w', 'james james james hwang'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaf39f8",
   "metadata": {},
   "source": [
    "# re.finditer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "194feb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<callable_iterator object at 0x104addf40>\n",
      "<re.Match object; span=(0, 1), match='1'>\n",
      "<re.Match object; span=(4, 5), match='2'>\n",
      "<re.Match object; span=(8, 9), match='3'>\n"
     ]
    }
   ],
   "source": [
    "iter1 =re.finditer('[\\d]', '1ab 2cd 3ef')\n",
    "print(iter1)\n",
    "for i in iter1:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd4c45a",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "32d66f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time', 'is', 'gold']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'time is gold'\n",
    "token = [x for x in s.split()]\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e0dc24",
   "metadata": {},
   "source": [
    "use `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "35e94c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9a4c0ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/siro/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c1fceb",
   "metadata": {},
   "source": [
    "# word tokenize\n",
    "\n",
    "use `nltk`'s `word_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6cfd91b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['time', 'is', 'gold']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(s)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d57902",
   "metadata": {},
   "source": [
    "# sentence tokenize\n",
    "\n",
    "`nltk`'s `sent_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "419d5805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world is a beautiful place.\n",
      "I want pizza.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['The world is a beautiful place.', 'I want pizza.']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = 'The world is a beautiful place.\\nI want pizza.'\n",
    "print(sentences)\n",
    "\n",
    "tokens = [x for x in sentences.split('\\n')]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "db8f9be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The world is a beautiful place.', 'I want pizza.']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "tokens = sent_tokenize(sentences)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "33feb4b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where', 'there', 's', 'a', 'will', 'there', 's', 'a', 'way']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "s = 'Where there\\'s a will, there\\'s a way'\n",
    "\n",
    "tokenizer = RegexpTokenizer('[\\w]+')\n",
    "\n",
    "tokens = tokenizer.tokenize(s)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "67831184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Where', \"there's\", 'a', 'will,', \"there's\", 'a', 'way']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2 = RegexpTokenizer('[\\s]+', gaps=True)\n",
    "tokens = tokenizer2.tokenize(s)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6e99a143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where', \"there's\", 'a', 'will', \"there's\", 'a', 'way']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "s = 'Where there\\'s a will, there\\'s a way'\n",
    "\n",
    "text_to_word_sequence(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "04291d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('There', 'is'),\n",
       " ('is', 'no'),\n",
       " ('no', 'royal'),\n",
       " ('royal', 'road'),\n",
       " ('road', 'to'),\n",
       " ('to', 'learning')]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "s = 'There is no royal road to learning'\n",
    "bigram = list(ngrams(s.split(), 2))\n",
    "bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "032393e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Think',\n",
       " 'like',\n",
       " 'man',\n",
       " 'of',\n",
       " 'action',\n",
       " 'and',\n",
       " 'act',\n",
       " 'like',\n",
       " 'man',\n",
       " 'of',\n",
       " 'thought',\n",
       " '.']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = word_tokenize(\"Think like man of action and act like man of thought.\")\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "409d10dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/siro/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Think', 'VBP'),\n",
       " ('like', 'IN'),\n",
       " ('man', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('action', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('act', 'NN'),\n",
       " ('like', 'IN'),\n",
       " ('man', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('thought', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1fc0b123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " ('rolling', 'VBG'),\n",
       " ('stone', 'NN'),\n",
       " ('gathers', 'NNS'),\n",
       " ('no', 'DT'),\n",
       " ('moss', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(word_tokenize(\"A rolling stone gathers no moss.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "15045b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/siro/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f1b82164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "21e756d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If', 'walk', 'today', ',', 'run', 'tomorrow', '.']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"If you do not walk today, you will have to run tomorrow.\"\n",
    "\n",
    "words = word_tokenize(s)\n",
    "\n",
    "non_stop = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        non_stop.append(w)\n",
    "non_stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aed635",
   "metadata": {},
   "source": [
    "# Spell check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09b1a087",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'autocorrect'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/siro/Projects/Python/ml/nlp/nltk-stuff/nltk-1.ipynb Cell 40'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/siro/Projects/Python/ml/nlp/nltk-stuff/nltk-1.ipynb#ch0000039?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mautocorrect\u001b[39;00m \u001b[39mimport\u001b[39;00m Speller\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autocorrect'"
     ]
    }
   ],
   "source": [
    "from autocorrect import Speller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "186b485b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people\n",
      "people\n",
      "people\n"
     ]
    }
   ],
   "source": [
    "spell = Speller('en')\n",
    "\n",
    "print(spell('peoplle'))\n",
    "print(spell('poeple'))\n",
    "print(spell('peopae'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8f66bc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Early bird catches the worm .'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = word_tokenize('Earlly biird catchees the womm.')\n",
    "ss = ' '.join([spell(s) for s in s])\n",
    "ss"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "af4893683edfb78f54f9960b469406bd48245d7e3b29654be734042f3de10859"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
